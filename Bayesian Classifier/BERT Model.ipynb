{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bertTemplate_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTBf3SoAUlXL",
        "outputId": "bc11b4e7-cc97-4db5-a0a1-3b7efa396223",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers\n",
        "# !unzip data_sets.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kxXyVR1xQ15",
        "outputId": "e26e650b-c513-4097-d3a1-a613dadf22e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "import random\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig, AutoTokenizer, TFBertModel\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "import heapq\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps = PorterStemmer()\n",
        "# BERT is pretrained with a max length of 512, you can define a lower max length ~ 300\n",
        "\n",
        "\n",
        "#I added 128 below\n",
        "MAX_LEN = 256\n",
        "PRETRAINED_MODEL_ID = \"bert-base-uncased\"\n",
        "tf.random.set_seed(\n",
        "    42\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiZhIeiG_V4v"
      },
      "source": [
        "def loadRawData(path):\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "    #Read in the negative reviews for training\n",
        "    for file in os.listdir(path+'/training_set/neg/'):\n",
        "        if file.endswith('.txt'):\n",
        "            with open(path+'/training_set/neg/'+file) as f:\n",
        "                raw = f.read()\n",
        "                x_train.append(raw)\n",
        "                y_train.append(0)\n",
        "\n",
        "    #Read in the positive reviews for training\n",
        "    for file in os.listdir(path + '/training_set/pos/'):\n",
        "        if file.endswith('.txt'):\n",
        "            with open(path + '/training_set/pos/' + file) as f:\n",
        "                raw = f.read()\n",
        "                x_train.append(raw)\n",
        "                y_train.append(1)\n",
        "\n",
        "    #Read in the negative reviews for testing\n",
        "    for file in os.listdir(path + '/test_set/neg/'):\n",
        "        if file.endswith('.txt'):\n",
        "            with open(path + '/test_set/neg/' + file) as f:\n",
        "                raw = f.read()\n",
        "                x_test.append(raw)\n",
        "                y_test.append(0)\n",
        "\n",
        "    #Read in the positive reviews for training\n",
        "    for file in os.listdir(path + '/test_set/pos/'):\n",
        "        if file.endswith('.txt'):\n",
        "            with open(path + '/test_set/pos/' + file) as f:\n",
        "                raw = f.read()\n",
        "                x_test.append(raw)\n",
        "                y_test.append(1)\n",
        "\n",
        "    # x_train = np.asarray(x_train)\n",
        "    # y_train = np.asarray(y_train)\n",
        "    # x_test = np.asarray(x_test)\n",
        "    # y_test = np.asarray(y_test)\n",
        "\n",
        "    shuffle = np.random.permutation(len(x_train))\n",
        "    x_train1 =[]\n",
        "    y_train1= []\n",
        "    for i in shuffle:\n",
        "        x_train1.append(x_train[i])\n",
        "        y_train1.append(y_train[i])\n",
        "    \n",
        "    x_train= x_train1\n",
        "    y_train = y_train1\n",
        "\n",
        "    return x_train, x_test, y_train, y_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAS9XKAtFCBd"
      },
      "source": [
        "The following function represents the reviews in a format accepted by BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeRpJGHc4LVe"
      },
      "source": [
        "### Already implemented\n",
        "def create_inputs_targets(text_examples, tokenizer):\n",
        "    '''converts inputs into the representation accepted by BERT\n",
        "    '''\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \n",
        "        \"attention_mask\": []\n",
        "    }\n",
        "    \n",
        "    for item in text_examples:\n",
        "        encodings = tokenizer(item, return_tensors='tf', max_length = MAX_LEN, truncation = True, pad_to_max_length=True)\n",
        "        \n",
        "        dataset_dict[\"input_ids\"].append((encodings.input_ids))\n",
        "        dataset_dict[\"attention_mask\"].append((encodings.attention_mask))\n",
        "    \n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.squeeze(np.array(dataset_dict[key]), axis=1)\n",
        "           \n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "     \n",
        "        dataset_dict[\"attention_mask\"]\n",
        "    ]\n",
        "    \n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfe59cKtFUuf"
      },
      "source": [
        " We will use TFBertForSequenceClassification. This interface  enables us to load a pretrained BERT model with an added untrained single linear layer on top for classification that we will use as a classifier. We will finetune the entire pre-trained BERT model and the additional untrained classification layer for our Movie Review classification task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARFGYxVI_Owk"
      },
      "source": [
        "## Already Implemented\n",
        "def create_bert(args):\n",
        "    '''Creates a model specified by pretrained_model_id\n",
        "\n",
        "    '''\n",
        "    # load pretrained BERT model \n",
        "    \n",
        "    \n",
        "    model = TFBertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_ID, num_labels = 2 )\n",
        "    \n",
        "    \n",
        "    # BERT inputs \n",
        "    input_ids = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
        "    \n",
        "    embedding = model(\n",
        "            input_ids,  attention_mask=attention_mask\n",
        "        )[0]\n",
        " \n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, attention_mask],\n",
        "        outputs=[embedding],\n",
        "    )\n",
        "    # from_logits=True computes the \"softmax\" activation as part of the cross-entropy loss layer\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    optimizer = keras.optimizers.Adam(lr=args['learning_rate'])\n",
        "    metrics = keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhNpDDiaGQtN"
      },
      "source": [
        "We will also compare to an MLP on BOW representation learnt in NaiveBayes Section. You can reuse code from HW3 for this portion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-JDNHmuGOys"
      },
      "source": [
        "def create_mlp(args=None):\n",
        "\t# You can use args to pass parameter values to this method\n",
        "\n",
        "\t# Define model architecture\n",
        "    input_shape = (100,)\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(256, activation = 'sigmoid', input_shape = input_shape))\n",
        "    model.add(keras.layers.Dense(128, activation= \"sigmoid\"))\n",
        "    model.add(keras.layers.Dense(32, activation= \"sigmoid\"))\n",
        "    model.add(keras.layers.Dense(1, activation= \"sigmoid\"))\n",
        "    # model = Sequential()\n",
        "\t# model.add(Dense(units=, activation=, input_dim=))\n",
        "\t# add more layers...\n",
        "\n",
        "\t# Optimizer\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(lr=args['learning_rate'])\n",
        "        \n",
        "    # Compile\n",
        "    model.compile(loss='binary_crossentropy', optimizer = keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    return model\n",
        "        \n",
        "\n",
        "\n",
        "def plot_history(history, plot_title=''):\n",
        "    train_loss_history = history.history['loss']\n",
        "    val_loss_history = history.history['val_loss']\n",
        "\n",
        "    train_acc_history = history.history['accuracy']\n",
        "    val_acc_history = history.history['val_accuracy']\n",
        "    # plot\n",
        "    plt.plot(train_loss_history)\n",
        "    plt.plot(val_loss_history)\n",
        "    plt.title('training loss and validation loss vs epoch number')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['train', 'test'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#changed validation_split to 0.1 from 0.0\n",
        "def train_model(x_train, y_train, model_type, validation_split=0.0, args=None):\n",
        "    # You can use args to pass parameter values to this method\n",
        "    if model_type == 'BERT':\n",
        "        model = create_bert(args)\n",
        "        history = model.fit((x_train[0],x_train[1]), y_train, epochs = 4, batch_size = 9, shuffle = True)\n",
        "    else:\n",
        "        model = create_mlp(args)\n",
        "        history = model.fit(x_train, y_train, epochs = 20, batch_size = 5, shuffle = True)\n",
        "    return model, history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRbQvue7AGhK"
      },
      "source": [
        "def train_and_select_model(x_train, y_train, model_type, grading_mode):\n",
        "    \"\"\"Optional method. You can write code here to perform a \n",
        "    parameter search, cross-validation, etc. \"\"\"\n",
        "\n",
        "    # you can define hyperparameters differently for BERT and MLP using if else. Note that for BERT, the architecture is fixed and no changes are required. \n",
        "    # you are free to change the learning rate, batch size, etc. for BERT. \n",
        "\n",
        "    args = {\n",
        "        'batch_size': 8,\n",
        "        'validation_split': 0.1,\n",
        "\t\t'epoch':4,\n",
        "        'learning_rate': 0.00001\n",
        "        }\n",
        "    \n",
        "    best_valid_acc = 0\n",
        "    best_hyper_set = {}\n",
        "    \n",
        "\n",
        "    # x_train1 = [x_train[i] for i in shuffle]\n",
        "    # y_train1 = [y_train[i] for i in shuffle] \n",
        "\n",
        "    ## Select best values for hyperparamters such as learning_rate, optimizer, hidden_layer, hidden_dim, regularization...\n",
        "    if not grading_mode:\n",
        "        if model_type == 'BERT':\n",
        "            best_model, best_history = train_model(x_train, y_train, model_type, validation_split=args['validation_split'], args=args)\n",
        "        else:\n",
        "            args = {\n",
        "            'batch_size': 8,\n",
        "            'validation_split': 0.1,\n",
        "\t\t    'epoch':4,\n",
        "            'learning_rate': 0.001\n",
        "            }\n",
        "            best_model, best_history = train_model(x_train, y_train, model_type, validation_split=args['validation_split'], args=args)                    \n",
        "        # for learning_rate in lr_values:\n",
        "        #     for opt in ['adam']:\n",
        "        #         for other_hyper in other_hyper_set:  ## search over other hyperparameters\n",
        "        #             args['opt'] = opt\n",
        "        #             args['learning_rate'] = learning_rate\n",
        "        #             args['other_hyper'] = other_hyper\t\t\t\t\n",
        "        #             model, history = train_model(x_train, y_train, model_type, validation_split=args['validation_split'], args=args)\n",
        "        #             validation_accuracy = history.history['val_accuracy']\t\t\t\t\t\t\t\t\t\n",
        "        #             max_valid_acc = max(validation_accuracy)\n",
        "        \n",
        "        #             if max_valid_acc > best_valid_acc:\n",
        "        #                 best_model = model\n",
        "        #                 best_valid_acc = max_valid_acc\n",
        "        #                 best_hyper_set['learning_rate'] = learning_rate\n",
        "        #                 best_hyper_set['opt'] = opt\n",
        "        #                 best_history = history\n",
        "    else:\n",
        "        if model_type == 'BERT':\n",
        "            best_model, best_history = train_model(x_train, y_train, model_type, validation_split=args['validation_split'], args=args)\n",
        "        else:\n",
        "            args = {\n",
        "            'batch_size': 8,\n",
        "            'validation_split': 0.1,\n",
        "\t\t    'epoch':4,\n",
        "            'learning_rate': 0.001\n",
        "            }\n",
        "            best_model, best_history = train_model(x_train, y_train, model_type, validation_split=args['validation_split'], args=args)\n",
        "        \n",
        "            \n",
        "        \n",
        "    return best_model, best_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEL2eRI-Sa9b"
      },
      "source": [
        "Now, we will train both models and compare performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezPCMSsq5pDQ"
      },
      "source": [
        "def processData(fullDatasetPath, model_type):\n",
        "  \n",
        "  # we will use the bert-base-uncased tokenizer - \n",
        "  if model_type == 'BERT':\n",
        "    # load datasets as lists of reviews\n",
        "    # no preprocessing required!\n",
        "    # we will use the bert-base-uncased tokenizer - \n",
        "    XtrainText, XtestText, ytrain, ytest = loadRawData(fullDatasetPath)\n",
        "    tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_ID)\n",
        "    \n",
        "    # tokenize for bert\n",
        "    xtrain = create_inputs_targets(XtrainText, tokenizer)\n",
        "    ytrain = np.array(ytrain)\n",
        "\n",
        "    xtest = create_inputs_targets(XtestText, tokenizer)\n",
        "    ytest = np.array(ytest)\n",
        "    \n",
        "  else:\n",
        "    # reuse data loading functions from NaiveBayes portions, you can optionally save the BOW matrices and reload here \n",
        "    XtrainText, XtestText, ytrain, ytest, vocabulary = loadData(fullDatasetPath)\n",
        "    MIN_FREQ = 3\n",
        "    MAX_VOCAB = 100\n",
        "    vocabulary = dict((word, index) for word, index in vocabulary.items() if\n",
        "                      vocabulary[word]>= MIN_FREQ and word in heapq.nlargest(MAX_VOCAB,\n",
        "                                                                                      vocabulary, key=vocabulary.get))\n",
        "    xtrain, xtest = getBOWRepresentation(XtrainText, XtestText, vocabulary) \n",
        "    \n",
        "  return xtrain, xtest, ytrain, ytest\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_XsC9BimPIk"
      },
      "source": [
        "# Finetune pretrained BERT and train MLP from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsZjNMGW57AF"
      },
      "source": [
        "def loadData(path):\n",
        "    '''\n",
        "    reads data from the folders\n",
        "    x_train : [review1, review2, ....., review_n], where each review1 is a list of tokens\n",
        "\n",
        "    vocabulary is a dictionary: (key: word, value: count)\n",
        "    '''\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    vocabulary = []\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "    #Read in the negative reviews for training\n",
        "    for file in os.listdir(path+'/training_set/neg/'):\n",
        "      if file.endswith('.txt'):\n",
        "          with open(path+'/training_set/neg/'+file) as fil:\n",
        "              raw = fil.read().strip()\n",
        "              unstemmed_tokens = tokenizer.tokenize(raw)\n",
        "              tokens1 = [ps.stem(word) for word in unstemmed_tokens]\n",
        "              tokens = [word for word in tokens1 if word not in stop_words]\n",
        "              vocabulary += tokens\n",
        "              x_train += [tokens]\n",
        "              y_train.append(0)\n",
        "\n",
        "    #Read in the positive reviews for training\n",
        "    for file in os.listdir(path+'/training_set/pos/'):\n",
        "      if file.endswith('.txt'):\n",
        "          with open(path+'/training_set/pos/'+file) as fil:\n",
        "              raw = fil.read().strip()\n",
        "              unstemmed_tokens = tokenizer.tokenize(raw)\n",
        "              tokens1 = [ps.stem(word) for word in unstemmed_tokens]\n",
        "              tokens = [word for word in tokens1 if word not in stop_words]\n",
        "              vocabulary += tokens\n",
        "              x_train += [tokens]\n",
        "              y_train.append(1)\n",
        "\n",
        "    #Read in the negative reviews for testing\n",
        "    for file in os.listdir(path+'/test_set/neg/'):\n",
        "      if file.endswith('.txt'):\n",
        "          with open(path+'/test_set/neg/'+file) as fil:\n",
        "              raw = fil.read().strip()\n",
        "              unstemmed_tokens = tokenizer.tokenize(raw)\n",
        "              tokens1 = [ps.stem(word) for word in unstemmed_tokens]\n",
        "              tokens = [word for word in tokens1 if word not in stop_words]\n",
        "              vocabulary += tokens\n",
        "              x_test += [tokens]\n",
        "              y_test.append(0)\n",
        "\n",
        "    #Read in the positive reviews for training\n",
        "    for file in os.listdir(path+'/test_set/pos/'):\n",
        "      if file.endswith('.txt'):\n",
        "          with open(path+'/test_set/pos/'+file) as fil:\n",
        "              raw = fil.read().strip()\n",
        "              unstemmed_tokens = tokenizer.tokenize(raw)\n",
        "              tokens1 = [ps.stem(word) for word in unstemmed_tokens]\n",
        "              tokens = [word for word in tokens1 if word not in stop_words]\n",
        "              vocabulary += tokens\n",
        "              x_test += [tokens]\n",
        "              y_test.append(1)\n",
        "\n",
        "    # Convert vocabulary list to dictionary\n",
        "    vocab_dict = {}\n",
        "    for word in vocabulary:\n",
        "      if word in vocab_dict:\n",
        "          vocab_dict[word] = vocab_dict[word]+1\n",
        "      else:\n",
        "          vocab_dict[word] = 1\n",
        "    shuffle = np.random.permutation(len(x_train))\n",
        "    x_train = [x_train[i] for i in shuffle]\n",
        "    y_train = [y_train[i] for i in shuffle]\n",
        "    return x_train, x_test, y_train, y_test, vocab_dict\n",
        "\n",
        "def getBOWRepresentation(x_train, x_test, vocabulary):\n",
        "    '''\n",
        "    converts into Bag of Words representation\n",
        "    each column is a feature(unique word) from the vocabulary\n",
        "    x_train_bow : a numpy array with bag of words representation\n",
        "    '''\n",
        "    x_train_bow = np.zeros([len(x_train), len(vocabulary)])\n",
        "    for i,review in enumerate(x_train):\n",
        "        for j,word in enumerate(vocabulary):\n",
        "            x_train_bow[i][j] = review.count(word)\n",
        "\n",
        "    x_test_bow = np.zeros([len(x_test), len(vocabulary)])\n",
        "    for i,review in enumerate(x_test):\n",
        "        for j,word in enumerate(vocabulary):\n",
        "            x_test_bow[i][j] = review.count(word)\n",
        "\n",
        "    return np.array(x_train_bow), np.array(x_test_bow)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IUKPt4pdxY4",
        "outputId": "44ca2214-ec9c-4b96-a12e-db74aab03042",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grading_mode = True\n",
        "path = '/content/drive/My Drive/data_sets/'\n",
        "\n",
        "if grading_mode:\n",
        "    # BERT model \n",
        "    xtrain, xtest, ytrain, ytest = processData(path, 'BERT')\n",
        "    model, history = train_and_select_model(xtrain, ytrain, 'BERT', grading_mode=True)\n",
        "    loss_test, acc_test = model.evaluate(xtest, ytest)\n",
        "    print(acc_test)\n",
        "    # MLP model\n",
        "    xtrain, xtest, ytrain, ytest = processData(path, 'MLP')\n",
        "    vec_length = len(xtrain[0])\n",
        "    xtrain = np.array(xtrain)\n",
        "    ytrain = np.array(ytrain)\n",
        "    xtest = np.array(xtest)\n",
        "    ytest = np.array(ytest)\n",
        "    model, history = train_and_select_model(xtrain, ytrain, 'MLP', grading_mode=True)\n",
        "    loss_test, acc_test = model.evaluate(xtest, ytest)\n",
        "    print(acc_test)\n",
        "\n",
        "else:\n",
        "  #BERT model \n",
        "    xtrain, xtest, ytrain, ytest = processData(path, 'BERT')\n",
        "    model, history = train_and_select_model(xtrain, ytrain, 'BERT', grading_mode=False)\n",
        "    loss_test, acc_test = model.evaluate(xtest, ytest)\n",
        "    plot_history(history)\n",
        "\n",
        "    #MLP model\n",
        "    xtrain, xtest, ytrain, ytest = processData(path, 'MLP')\n",
        "    vec_length = len(xtrain[0])\n",
        "    xtrain = np.array(xtrain)\n",
        "    ytrain = np.array(ytrain)\n",
        "    xtest = np.array(xtest)\n",
        "    ytest = np.array(ytest)\n",
        "    model, history = train_and_select_model(xtrain, ytrain, 'MLP', grading_mode=False)\n",
        "    loss_test, acc_test = model.evaluate(xtest, ytest)\n",
        "    plot_history(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dropout_113', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "156/156 [==============================] - 86s 554ms/step - loss: 0.6915 - accuracy: 0.5250\n",
            "Epoch 2/4\n",
            "156/156 [==============================] - 86s 552ms/step - loss: 0.5452 - accuracy: 0.7307\n",
            "Epoch 3/4\n",
            "156/156 [==============================] - 86s 554ms/step - loss: 0.3289 - accuracy: 0.8643\n",
            "Epoch 4/4\n",
            "156/156 [==============================] - 86s 553ms/step - loss: 0.1551 - accuracy: 0.9514\n",
            "19/19 [==============================] - 11s 576ms/step - loss: 0.4739 - accuracy: 0.8233\n",
            "0.8233333230018616\n",
            "Epoch 1/20\n",
            "280/280 [==============================] - 1s 2ms/step - loss: 0.6904 - accuracy: 0.5186\n",
            "Epoch 2/20\n",
            "280/280 [==============================] - 1s 2ms/step - loss: 0.6787 - accuracy: 0.6143\n",
            "Epoch 3/20\n",
            "280/280 [==============================] - 1s 2ms/step - loss: 0.6635 - accuracy: 0.6686\n",
            "Epoch 4/20\n",
            "280/280 [==============================] - 1s 2ms/step - loss: 0.6396 - accuracy: 0.6929\n",
            "Epoch 5/20\n",
            "280/280 [==============================] - 1s 2ms/step - loss: 0.6126 - accuracy: 0.6943\n",
            "Epoch 6/20\n",
            "280/280 [==============================] - 1s 2ms/step - loss: 0.5859 - accuracy: 0.7129\n",
            "Epoch 7/20\n",
            "280/280 [==============================] - 1s 2ms/step - loss: 0.5611 - accuracy: 0.7343\n",
            "Epoch 8/20\n",
            "280/280 [==============================] - 1s 2ms/step - loss: 0.5404 - accuracy: 0.7414\n",
            "Epoch 9/20\n",
            "280/280 [==============================] - 1s 2ms/step - loss: 0.5248 - accuracy: 0.7464\n",
            "Epoch 10/20\n",
            "280/280 [==============================] - 1s 2ms/step - loss: 0.5128 - accuracy: 0.7479\n",
            "Epoch 11/20\n",
            "280/280 [==============================] - 1s 2ms/step - loss: 0.5052 - accuracy: 0.7543\n",
            "Epoch 12/20\n",
            "280/280 [==============================] - 1s 2ms/step - loss: 0.4979 - accuracy: 0.7557\n",
            "Epoch 13/20\n",
            "280/280 [==============================] - 1s 2ms/step - loss: 0.4962 - accuracy: 0.7493\n",
            "Epoch 14/20\n",
            "280/280 [==============================] - 1s 2ms/step - loss: 0.4894 - accuracy: 0.7500\n",
            "Epoch 15/20\n",
            "280/280 [==============================] - 0s 2ms/step - loss: 0.4840 - accuracy: 0.7586\n",
            "Epoch 16/20\n",
            "280/280 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.7557\n",
            "Epoch 17/20\n",
            "280/280 [==============================] - 0s 2ms/step - loss: 0.4780 - accuracy: 0.7571\n",
            "Epoch 18/20\n",
            "280/280 [==============================] - 0s 2ms/step - loss: 0.4765 - accuracy: 0.7579\n",
            "Epoch 19/20\n",
            "280/280 [==============================] - 0s 2ms/step - loss: 0.4739 - accuracy: 0.7579\n",
            "Epoch 20/20\n",
            "280/280 [==============================] - 0s 2ms/step - loss: 0.4715 - accuracy: 0.7621\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.5021 - accuracy: 0.7450\n",
            "0.7450000047683716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtqR1bvzy8Rg"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}